{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33243c47",
   "metadata": {},
   "source": [
    "## saving model using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b43313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model_pickle','wb') as file:\n",
    "    pickle.dump(model,file)\n",
    "with open('model_pickle','rb') as file:\n",
    "    mp = pickle.load(file)\n",
    "mp.predict([[5000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04785df6",
   "metadata": {},
   "source": [
    "## saving model using joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'model_joblib')\n",
    "mj = joblib.load('model_joblib')\n",
    "mj.predict([[5000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7070b6fd",
   "metadata": {},
   "source": [
    "## Using pandas to create dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ae5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dummies = pd.get_dummies(df.town)\n",
    "merged = pd.concat([df,dummies],axis='columns')\n",
    "final = merged.drop(['town'], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e2ce4d",
   "metadata": {},
   "source": [
    "## hot label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bda476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df.town = le.fit_transform(df.town)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388fcbe",
   "metadata": {},
   "source": [
    "## using one hot encoding to create dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "ct = ColumnTransformer([('town', OneHotEncoder(), [0])], remainder = 'passthrough')\n",
    "X = ct.fit_transform(X)\n",
    "# here x is an 2d array in which columns are features and rows are rows..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218766e8",
   "metadata": {},
   "source": [
    "## train test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8009509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267798d2",
   "metadata": {},
   "source": [
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c255c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c0f326",
   "metadata": {},
   "source": [
    "## stratified shuffle split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00672296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here there is a problem, for eg CHAS hos only two values 0 and 1, 0-475 and 1-35\n",
    "# but in spliting train and test data suppose traindata = 402, testdata=104\n",
    "# if there are no 1's in train data then our program forms wrong pattern that \n",
    "# there is only one possibility of CHAS i.e, 1\n",
    "# to solve above problem\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing['CHAS']):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efda1d4",
   "metadata": {},
   "source": [
    "## looking for Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab8d400",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix['label_name'].sort_values(ascending=False)\n",
    "# if value is 1 meaning strong positive correlation\n",
    "# if value is -1 meaning strong negative correlation\n",
    "# next Rm value is 0.69 which is high positive correlation which means if RM increases chances\n",
    "# of increasing MEDV increases, then ZN and B are weak positive correlation\n",
    "# similarly lstat is high neg corr, lesser value of lstat higher value of medv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a388cb9d",
   "metadata": {},
   "source": [
    "## Missing attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788b6f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To take care of missing attributes, you have three options:\n",
    "#     1. Get rid of the missing data points\n",
    "    a=housing.dropna(subset=[\"RM\"])\n",
    "\n",
    "#     2. Get rid of the whole attribute\n",
    "    housing.drop(\"RM\", axis=1).shape \n",
    "\n",
    "#     3. Set the value to some value(0, mean or median)\n",
    "    housing[\"RM\"].fillna(housing[\"RM\"].median())\n",
    "    housing[\"RM\"].fillna(housing[\"RM\"].mean())\n",
    "    housing[\"RM\"].fillna(housing[\"RM\"].mode()[0])# for object type\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5944329",
   "metadata": {},
   "source": [
    "## Creating a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecead117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of doing imputer, directly you can opt for pipeline which automates things\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "my_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "#     add as many as you want...\n",
    "    ('std_scaler',StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8fd5ad",
   "metadata": {},
   "source": [
    "## Using better evaluation techniques - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for eg 1 2 3 4 5 6 7 8 9, firstly it will train expect 1 and check for 1, then repeat it for 2, 3..\n",
    "# so on.. to last value\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, housing_num_tr, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a32a39",
   "metadata": {},
   "source": [
    "## Min max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f39a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(df[['Income($)']])\n",
    "df['Income($)'] = scaler.transform(df[['Income($)']])\n",
    "\n",
    "scaler.fit(df[['Age']])\n",
    "df['Age'] = scaler.transform(df[['Age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb6c79",
   "metadata": {},
   "source": [
    "## Count vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c67e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert sentences in data frame to some numbers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "v = CountVectorizer()\n",
    "X_train_count = v.fit_transform(X_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2793ee",
   "metadata": {},
   "source": [
    "## hyper parameter tuning with grid search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f1fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model_params = {\n",
    "    'svm': {\n",
    "        'model': svm.SVC(gamma='auto'),\n",
    "        'params' : {\n",
    "            'C': [1,10,20],\n",
    "            'kernel': ['rbf','linear']\n",
    "        }  \n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params' : {\n",
    "            'n_estimators': [1,5,10]\n",
    "        }\n",
    "    },\n",
    "    'logistic_regression' : {\n",
    "        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n",
    "        'params': {\n",
    "            'C': [1,5,10]\n",
    "        }\n",
    "    },\n",
    "    'naive_bayes_gaussian': {\n",
    "        'model': GaussianNB(),\n",
    "        'params': {}\n",
    "    },\n",
    "    'naive_bayes_multinomial': {\n",
    "        'model': MultinomialNB(),\n",
    "        'params': {}\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'criterion': ['gini','entropy'],\n",
    "            \n",
    "        }\n",
    "    }     \n",
    "}\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n",
    "    clf.fit(digits.data, digits.target)\n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': clf.best_score_,\n",
    "        'best_params': clf.best_params_\n",
    "    })\n",
    "    \n",
    "df = pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255449f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
